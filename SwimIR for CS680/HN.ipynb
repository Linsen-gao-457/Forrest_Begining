{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "\n",
    "# For BM25 hard negatives\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Preprocess dataset for hard negative mining\n",
    "def preprocess_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Process the MIRACL dev split to create queries, positives, and corpus.\n",
    "    \n",
    "    Returns:\n",
    "        queries (dict): {query_id: query_text}\n",
    "        positives (dict): {query_id: positive_docid}\n",
    "        corpus (dict): {passage_id: passage_text}\n",
    "    \"\"\"\n",
    "    data = dataset[\"dev\"]\n",
    "    \n",
    "    # Create queries dictionary: {query_id: query_text}\n",
    "    queries = {str(item[\"query_id\"]): item[\"query\"] for item in data}\n",
    "    \n",
    "    # Create positives dictionary: {query_id: positive_docid}\n",
    "    positives = {}\n",
    "    relevant_docids = set()\n",
    "    for item in data:\n",
    "        query_id = str(item[\"query_id\"])\n",
    "        if item[\"positive_passages\"]:\n",
    "            positives[query_id] = str(item[\"positive_passages\"][0][\"docid\"])\n",
    "        for passage in item[\"negative_passages\"]:\n",
    "            relevant_docids.add(str(passage[\"docid\"]))\n",
    "    \n",
    "    # Load corpus from miracl/miracl-corpus for Yoruba\n",
    "    corpus_dataset = load_dataset(\"miracl/miracl-corpus\", \"yo\", trust_remote_code=True)[\"train\"]\n",
    "    corpus = {str(item[\"docid\"]): item[\"text\"] for item in corpus_dataset}\n",
    "    \n",
    "    return queries, positives, corpus\n",
    "\n",
    "def bm25_retrieve(query_text, corpus_texts, corpus_ids, top_k=20):\n",
    "    \"\"\"\n",
    "    Retrieve hard negatives using BM25 and normalize scores to [0, 1].\n",
    "    \"\"\"\n",
    "    tokenize = lambda text: text.lower().split()\n",
    "    tokenized_corpus = [tokenize(text) for text in corpus_texts]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    tokenized_query = tokenize(query_text)\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Get indices sorted by BM25 score (highest first)\n",
    "    sorted_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    # Define results with raw scores first\n",
    "    results = [(corpus_ids[idx], scores[idx], idx) for idx in sorted_indices]\n",
    "    \n",
    "    # Normalize BM25 scores to [0, 1]\n",
    "    bm25_scores = [item[1] for item in results]  # Extract raw scores from results\n",
    "    min_score, max_score = min(bm25_scores), max(bm25_scores)\n",
    "    if max_score > min_score:  # Avoid division by zero\n",
    "        results = [(docid, (score - min_score) / (max_score - min_score), idx) \n",
    "                   for docid, score, idx in results]\n",
    "    else:\n",
    "        results = [(docid, 1.0, idx) for docid, score, idx in results]  # All scores equal\n",
    "    logger.info(\"BM25 finish 1 iterate\")\n",
    "    return results\n",
    "\n",
    "def kalm_retrieve(query_embedding, corpus_embeddings, corpus_ids, top_k=20):\n",
    "    \"\"\"\n",
    "    Retrieve hard negatives using pre-computed KaLM embeddings with FAISS.\n",
    "    \"\"\"\n",
    "    # Normalize embeddings\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    faiss.normalize_L2(corpus_embeddings)\n",
    "    \n",
    "    dim = corpus_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(corpus_embeddings)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for score, idx in zip(distances[0], indices[0]):\n",
    "        results.append((corpus_ids[idx], score, idx))\n",
    "    logger.info(\"KaLM finish 1 iterate\")\n",
    "    return results\n",
    "\n",
    "# Hard negative mining combining dense, BM25, and KaLM strategies\n",
    "def mine_hard_negatives(model, queries, positives, corpus_dict, bm25_top_k=30, kalm_top_k=30, negatives_to_mine=15):\n",
    "    \"\"\"\n",
    "    Mine hard negatives for each query using a combination of BM25 and KaLM-based scoring.\n",
    "    \n",
    "    Args:\n",
    "        model: Preloaded SentenceTransformer model for KaLM retrieval.\n",
    "    \n",
    "    Returns:\n",
    "        training_batches (list): List of dicts with query, positive, and hard negative info.\n",
    "    \"\"\"\n",
    "    # Prepare corpus lists\n",
    "    corpus_ids = list(corpus_dict.keys())\n",
    "    corpus_texts = list(corpus_dict.values())\n",
    "    query_ids = list(queries.keys())\n",
    "\n",
    "    # Pre-compute corpus embeddings once to save time\n",
    "    corpus_embeddings = model.encode(corpus_texts, convert_to_numpy=True)\n",
    "    faiss.normalize_L2(corpus_embeddings)\n",
    "\n",
    "    training_batches = []\n",
    "    for qid in query_ids:\n",
    "        gold_docid = positives.get(qid)\n",
    "        if gold_docid not in corpus_ids:\n",
    "            # Skip if the positive document is not in the corpus.\n",
    "            continue\n",
    "        \n",
    "        # BM25 negatives\n",
    "        logger.info(\"BM25\")\n",
    "        bm25_results = bm25_retrieve(queries[qid], corpus_texts, corpus_ids, top_k=bm25_top_k)\n",
    "        bm25_results = [item for item in bm25_results if item[0] != gold_docid]\n",
    "        \n",
    "        # KaLM negatives (use pre-computed corpus embeddings)\n",
    "        logger.info(\"KaLM\")\n",
    "        query_embedding = model.encode([queries[qid]], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        kalm_results = kalm_retrieve(query_embedding, corpus_embeddings, corpus_ids, top_k=kalm_top_k)\n",
    "        kalm_results = [item for item in kalm_results if item[0] != gold_docid]\n",
    "        logger.info('Finish retrieval')\n",
    "        \n",
    "        # Merge negatives from BM25 and KaLM using a dictionary to deduplicate scores\n",
    "        neg_dict = {}\n",
    "        for source in [bm25_results, kalm_results]:\n",
    "            for docid, score, idx in source:\n",
    "                # If a document appears from both methods, take the higher score\n",
    "                neg_dict[docid] = max(neg_dict.get(docid, float('-inf')), score)\n",
    "        \n",
    "        # Sort negatives by score (descending) and take the top negatives_to_mine\n",
    "        sorted_negatives = sorted(neg_dict.items(), key=lambda x: x[1], reverse=True)[:negatives_to_mine]\n",
    "        hard_negative_ids = [docid for docid, _ in sorted_negatives]\n",
    "        hard_negative_scores = [neg_dict[docid] for docid in hard_negative_ids]\n",
    "\n",
    "        batch = {\n",
    "            \"query_id\": qid,\n",
    "            \"query_text\": queries[qid],\n",
    "            \"positive_id\": gold_docid,\n",
    "            \"hard_negative_ids\": hard_negative_ids,\n",
    "            \"hard_negative_scores\": hard_negative_scores\n",
    "        }\n",
    "        training_batches.append(batch)\n",
    "    \n",
    "    return training_batches\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "    \n",
    "    logger.info(\"Load model\")\n",
    "    \n",
    "    # Load model for dense retrieval\n",
    "    model_name = \"HIT-TMG/KaLM-embedding-multilingual-mini-v1\"\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "    logger.info(\"Load dataset\")\n",
    "\n",
    "    # Load MIRACL dataset (Yoruba dev split)\n",
    "    dataset_name = \"miracl/miracl\"\n",
    "    dataset = load_dataset(dataset_name, \"yo\", trust_remote_code=True)\n",
    "    \n",
    "    logger.info(\"Preprocess dataset\")\n",
    "    # Preprocess dataset\n",
    "    queries, positives, corpus_dict = preprocess_dataset(dataset)\n",
    "    \n",
    "    logger.info(\"Hard negative mining\")\n",
    "    \n",
    "    # Mine hard negatives using dense, BM25, and KaLM methods\n",
    "    training_batches = mine_hard_negatives(model, queries, positives, corpus_dict, \n",
    "                                          bm25_top_k=30, kalm_top_k=30, negatives_to_mine=15)\n",
    "    \n",
    "    # Print sample results\n",
    "    print(f\"\\nMined {len(training_batches)} training batches.\")\n",
    "    for batch in training_batches[:2]:  # Show first 2 batches\n",
    "        print(f\"\\nQuery ID: {batch['query_id']}\")\n",
    "        print(f\"Query Text: {batch['query_text']}\")\n",
    "        print(f\"Positive ID: {batch['positive_id']}\")\n",
    "        print(f\"Hard Negatives Count: {len(batch['hard_negative_ids'])}\")\n",
    "        print(f\"Hard Negative IDs: {batch['hard_negative_ids']}\")\n",
    "        print(f\"Hard Negative Scores: {batch['hard_negative_scores']}\")\n",
    "    \n",
    "    # Save batches for training\n",
    "    np.save(\"miracl_yo_training_batches.npy\", training_batches, allow_pickle=True)\n",
    "    print(\"\\nSaved training batches to 'miracl_yo_training_batches.npy'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mined 119 training batches.\n",
    "\n",
    "```Query ID: 10020#0\n",
    "Query Text: Odun wo ni wọn ṣe idije Olympiiki akọkọ?\n",
    "Positive ID: 10020#1\n",
    "Hard Negatives Count: 15\n",
    "Hard Negative IDs: ['61510#0', '67605#2', '10020#0', '54580#0', '53588#0', '54579#0', '54582#0', '54581#0', '10019#0', '54589#0', '54588#0', '54590#0', '54583#0', '54591#0', '54597#0']\n",
    "Hard Negative Scores: [1.0, 0.8826476173871683, 0.7056303, 0.6600299, 0.6508212, 0.64164484, 0.63624036, 0.63371813, 0.632797, 0.6303463, 0.6296907, 0.62422645, 0.62160224, 0.620029, 0.6162815]\n",
    "```\n",
    "```Query ID: 10118#0\n",
    "Query Text: Orile ede wo ni Washington DC wà?\n",
    "Positive ID: 55716#0\n",
    "Hard Negatives Count: 15\n",
    "Hard Negative IDs: ['68969#3', '3280#0', '66970#0', '20051#0', '13619#0', '70132#0', '67978#0', '18364#0', '66836#0', '71410#0', '22522#0', '1835#5', '16271#0', '67976#0', '1835#0']\n",
    "Hard Negative Scores: [1.0, 0.8165482099800115, 0.7664398104753086, 0.7495139429683063, 0.7296947, 0.7291269635579585, 0.6989294197861712, 0.6839287908370626, 0.6579214304185333, 0.6550035706510758, 0.6037975, 0.58926904, 0.57326627, 0.5725800646653812, 0.5608194]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
