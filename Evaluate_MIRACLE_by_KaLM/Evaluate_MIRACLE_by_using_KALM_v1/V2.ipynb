{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import pytrec_eval\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --------------------------\n",
    "# Our Evaluation Class\n",
    "# --------------------------\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class Evaluator:\n",
    "    @staticmethod\n",
    "    def evaluate(\n",
    "        qrels: dict[str, dict[str, int]],\n",
    "        results: dict[str, dict[str, float]],\n",
    "        k_values: list[int],\n",
    "        ignore_identical_ids: bool = True,\n",
    "    ) -> tuple[dict[str, float], dict[str, float], dict[str, float], dict[str, float]]:\n",
    "        if ignore_identical_ids:\n",
    "            logger.info(\n",
    "                \"For evaluation, we ignore identical query and document ids (default), please explicitly set ``ignore_identical_ids=False`` to not ignore them.\"\n",
    "            )\n",
    "            for qid, rels in results.items():\n",
    "                # Remove any document id that is identical to the query id\n",
    "                for pid in list(rels):\n",
    "                    if qid == pid:\n",
    "                        results[qid].pop(pid)\n",
    "\n",
    "        ndcg = {}\n",
    "        _map = {}\n",
    "        recall = {}\n",
    "        precision = {}\n",
    "\n",
    "        for k in k_values:\n",
    "            ndcg[f\"NDCG@{k}\"] = 0.0\n",
    "            _map[f\"MAP@{k}\"] = 0.0\n",
    "            recall[f\"Recall@{k}\"] = 0.0\n",
    "            precision[f\"P@{k}\"] = 0.0\n",
    "\n",
    "        map_string = \"map_cut.\" + \",\".join([str(k) for k in k_values])\n",
    "        ndcg_string = \"ndcg_cut.\" + \",\".join([str(k) for k in k_values])\n",
    "        recall_string = \"recall.\" + \",\".join([str(k) for k in k_values])\n",
    "        precision_string = \"P.\" + \",\".join([str(k) for k in k_values])\n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "            qrels, {map_string, ndcg_string, recall_string, precision_string}\n",
    "        )\n",
    "        scores = evaluator.evaluate(results)\n",
    "\n",
    "        for query_id in scores.keys():\n",
    "            for k in k_values:\n",
    "                ndcg[f\"NDCG@{k}\"] += scores[query_id].get(\"ndcg_cut_\" + str(k), 0)\n",
    "                _map[f\"MAP@{k}\"] += scores[query_id].get(\"map_cut_\" + str(k), 0)\n",
    "                recall[f\"Recall@{k}\"] += scores[query_id].get(\"recall_\" + str(k), 0)\n",
    "                precision[f\"P@{k}\"] += scores[query_id].get(\"P_\" + str(k), 0)\n",
    "\n",
    "        num_queries = len(scores)\n",
    "        for k in k_values:\n",
    "            ndcg[f\"NDCG@{k}\"] = round(ndcg[f\"NDCG@{k}\"] / num_queries, 5)\n",
    "            _map[f\"MAP@{k}\"] = round(_map[f\"MAP@{k}\"] / num_queries, 5)\n",
    "            recall[f\"Recall@{k}\"] = round(recall[f\"Recall@{k}\"] / num_queries, 5)\n",
    "            precision[f\"P@{k}\"] = round(precision[f\"P@{k}\"] / num_queries, 5)\n",
    "\n",
    "        for metric in [ndcg, _map, recall, precision]:\n",
    "            logger.info(\"\\nEvaluation metrics:\")\n",
    "            for key in metric.keys():\n",
    "                logger.info(f\"{key}: {metric[key]:.4f}\")\n",
    "\n",
    "        return ndcg, _map, recall, precision\n",
    "\n",
    "# --------------------------\n",
    "# Retrieval and Evaluation\n",
    "# --------------------------\n",
    "def get_embedding(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings using the loaded model.\n",
    "    \"\"\"\n",
    "    return model.encode(texts, convert_to_numpy=True, device=device)\n",
    "\n",
    "def preprocess_dataset(dataset, num_queries=None):\n",
    "    data = dataset[\"train\"]  # Use the appropriate split\n",
    "    if num_queries is None:\n",
    "        num_queries = len(data[\"title\"])\n",
    "    \n",
    "    queries = data[\"title\"][:num_queries]\n",
    "    query_ids = data[\"docid\"][:num_queries]  # Assuming each query has a unique docid\n",
    "    corpus = data[\"text\"]\n",
    "    \n",
    "    # Assume that for each query, the positive document is the corresponding corpus text\n",
    "    query_to_positive = {qid: {doc} for qid, doc in zip(query_ids, data[\"text\"][:num_queries])}\n",
    "    return queries, query_ids, corpus, query_to_positive\n",
    "\n",
    "def build_faiss_index(corpus_embeddings):\n",
    "    \"\"\"\n",
    "    Builds a FAISS index for efficient retrieval.\n",
    "    \"\"\"\n",
    "    dim = corpus_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(corpus_embeddings)\n",
    "    return index\n",
    "\n",
    "def evaluate_retrieval(dataset, lang, num_queries=None):\n",
    "    \"\"\"\n",
    "    Evaluate the retrieval model and compute NDCG@10 and Recall@100.\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating language: {lang}...\")\n",
    "    \n",
    "    # Preprocess dataset\n",
    "    queries, query_ids, corpus, query_to_positive = preprocess_dataset(dataset, num_queries)\n",
    "    if not queries or not corpus:\n",
    "        print(f\"No valid data found for {lang}\")\n",
    "        return 0, 0\n",
    "\n",
    "    # Embed queries and corpus\n",
    "    query_embeddings = get_embedding(queries)\n",
    "    corpus_embeddings = get_embedding(corpus)\n",
    "    \n",
    "    # Build FAISS index\n",
    "    index = build_faiss_index(corpus_embeddings)\n",
    "    \n",
    "    # Determine the maximum number of retrieved items needed\n",
    "    max_k = max(10, 100)\n",
    "    distances, indices = index.search(query_embeddings, max_k)\n",
    "    \n",
    "    # Format the results for pytrec_eval:\n",
    "    # Use corpus texts as document ids. (In your qrels, you use the actual corpus text.)\n",
    "    results = {}\n",
    "    for i, qid in enumerate(query_ids):\n",
    "        results[qid] = {\n",
    "            corpus[idx]: float(-distances[i][j])  # Negative distance as score\n",
    "            for j, idx in enumerate(indices[i][:max_k])\n",
    "        }\n",
    "    \n",
    "    # Format the qrels. Each query has a set of relevant documents (here, the positive document).\n",
    "    qrels = {\n",
    "        qid: {doc: 1 for doc in query_to_positive.get(qid, [])} \n",
    "        for qid in query_ids\n",
    "    }\n",
    "    \n",
    "    # Evaluate using our Evaluator class with k_values of 10 and 100.\n",
    "    k_values = [10, 100]\n",
    "    ndcg, _map, recall, precision = Evaluator.evaluate(qrels, results, k_values)\n",
    "    \n",
    "    # Extract the desired metrics\n",
    "    ndcg_at_10 = ndcg.get(\"NDCG@10\", 0)\n",
    "    recall_at_100 = recall.get(\"Recall@100\", 0)\n",
    "    print(f\"\\n{lang} - NDCG@10: {ndcg_at_10:.4f}, Recall@100: {recall_at_100:.4f}\")\n",
    "    \n",
    "    return recall_at_100, ndcg_at_10\n",
    "\n",
    "# --------------------------\n",
    "# Main Code: Load Model and Dataset\n",
    "# --------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model_name = \"facebook/mcontriever-msmarco\"\n",
    "model = SentenceTransformer(model_name, device=device)\n",
    "\n",
    "dataset_name = \"miracl/miracl-corpus\"\n",
    "# For example, use the Yoruba version. Adjust the configuration if needed.\n",
    "dataset = load_dataset(dataset_name, \"yo\", trust_remote_code=True)\n",
    "lang = \"Yoruba (yo)\"\n",
    "\n",
    "# Run evaluation\n",
    "recall, ndcg = evaluate_retrieval(dataset, lang)\n",
    "print(f\"\\nFinal Results:\\nRecall@100: {recall:.4f}\\nNDCG@10: {ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating language: Yoruba (yo)...\n",
    "Batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:13<00:00,  1.96s/it]\n",
    "Batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1533/1533 [01:09<00:00, 22.07it/s]\n",
    "INFO:__main__:For evaluation, we ignore identical query and document ids (default), please explicitly set ``ignore_identical_ids=False`` to not ignore them.\n",
    "INFO:__main__:\n",
    "Evaluation metrics:\n",
    "INFO:__main__:NDCG@10: 0.0282\n",
    "INFO:__main__:NDCG@100: 0.0295\n",
    "INFO:__main__:\n",
    "Evaluation metrics:\n",
    "INFO:__main__:MAP@10: 0.0275\n",
    "INFO:__main__:MAP@100: 0.0279\n",
    "INFO:__main__:\n",
    "Evaluation metrics:\n",
    "INFO:__main__:Recall@10: 0.0300\n",
    "INFO:__main__:Recall@100: 0.0350\n",
    "INFO:__main__:\n",
    "Evaluation metrics:\n",
    "INFO:__main__:P@10: 0.0030\n",
    "INFO:__main__:P@100: 0.0003\n",
    "\n",
    "Yoruba (yo) - NDCG@10: 0.0282, Recall@100: 0.0350\n",
    "\n",
    "Final Results:\n",
    "Recall@100: 0.0350\n",
    "NDCG@10: 0.0282\n",
    "l78gao@cdr2644 ~/scratch/Evaluate $ python test.py\n",
    "Using device: cuda\n",
    "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: facebook/mcontriever-msmarco\n",
    "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name facebook/mcontriever-msmarco. Creating a new one with mean pooling.\n",
    "Some weights of BertModel were not initialized from the model checkpoint at facebook/mcontriever-msmarco and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "\n",
    "Evaluating language: Yoruba (yo)...\n",
    "Batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1533/1533 [00:33<00:00, 45.61it/s]\n",
    "Batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 1533/1533 [01:09<00:00, 22.16it/s]\n",
    "INFO:__main__:For evaluation, we ignore identical query and document ids (default), please explicitly set ``ignore_identical_ids=False`` to not ignore them.\n",
    "INFO:__main__:\n",
    "Evaluation metrics:\n",
    "INFO:__main__:NDCG@10: 0.4726\n",
    "INFO:__main__:NDCG@100: 0.4822\n",
    "INFO:__main__:\n",
    "Evaluation metrics:\n",
    "INFO:__main__:MAP@10: 0.4621\n",
    "INFO:__main__:MAP@100: 0.4639\n",
    "INFO:__main__:\n",
    "Evaluation metrics:\n",
    "INFO:__main__:Recall@10: 0.5053\n",
    "INFO:__main__:Recall@100: 0.5531\n",
    "INFO:__main__:\n",
    "Evaluation metrics:\n",
    "INFO:__main__:P@10: 0.0505\n",
    "INFO:__main__:P@100: 0.0055\n",
    "\n",
    "Yoruba (yo) - NDCG@10: 0.4726, Recall@100: 0.5531\n",
    "\n",
    "Final Results:\n",
    "Recall@100: 0.5531\n",
    "NDCG@10: 0.4726"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
