{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEuV0FH64Vys"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load KaLM multilingual embedding model\n",
        "model_name = \"HIT-TMG/KaLM-embedding-multilingual-mini-v1\"\n",
        "model = SentenceTransformer(model_name).to(device)\n",
        "# Define function to get sentence embeddings\n",
        "def get_embedding(texts):\n",
        "    return model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
        "\n",
        "# Compute Recall@100 and NDCG@10 for a single language\n",
        "def evaluate_miracl(dataset):\n",
        "    print(f\"\\nEvaluating language: {lang}...\")\n",
        "\n",
        "    # Extract queries and positive passages\n",
        "    queries = dataset[\"query\"][:200]  # Extract the \"query\" field for the first 200 rows\n",
        "    positive_passages = [\n",
        "        passage[0] if len(passage) > 0 else None\n",
        "        for passage in dataset[\"positive_passages\"][:5000]  # Extract the first positive passage\n",
        "    ]\n",
        "\n",
        "    # Filter out rows where positive_passages is None\n",
        "    valid_data = [(q, p) for q, p in zip(queries, positive_passages) if p is not None]\n",
        "    if len(valid_data) == 0:\n",
        "        print(f\"No valid data found for {lang}\")\n",
        "        return 0, 0\n",
        "\n",
        "    # Separate queries and passages\n",
        "    queries, positive_passages = zip(*valid_data)\n",
        "\n",
        "    # Get embeddings\n",
        "    query_embeddings = get_embedding(list(queries))\n",
        "    passage_embeddings = get_embedding(list(positive_passages))\n",
        "\n",
        "    # Build FAISS index\n",
        "    dim = query_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(passage_embeddings)\n",
        "\n",
        "    # Retrieve top-k results\n",
        "    k_recall = 100\n",
        "    k_ndcg = 10\n",
        "    D, I = index.search(query_embeddings, max(k_recall, k_ndcg))\n",
        "\n",
        "    # Compute Recall@100\n",
        "    recall_count = sum(\n",
        "        [1 if idx in I[i][:k_recall] else 0 for i, idx in enumerate(range(len(queries)))]\n",
        "    )\n",
        "    recall_100 = recall_count / len(queries)\n",
        "\n",
        "    # Compute NDCG@10\n",
        "    relevance = np.zeros((len(queries), k_ndcg))\n",
        "    for i in range(len(queries)):\n",
        "        relevance[i, 0] = 1  # Assume the top-ranked result is relevant\n",
        "    ndcg_10 = np.mean(\n",
        "        [ndcg_score([rel], [rank]) for rel, rank in zip(relevance, I[:, :k_ndcg])]\n",
        "    )\n",
        "\n",
        "    print(f\"{lang} - Recall@100: {recall_100:.4f}, NDCG@10: {ndcg_10:.4f}\")\n",
        "    return recall_100, ndcg_10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the subset of languages to evaluate (from \"ko\" to \"ru\")\n",
        "languages = [\"ar\", \"bn\", \"en\", \"es\", \"fa\", \"fi\", \"fr\", \"hi\", \"id\", \"ja\", \"ko\", \"ru\", \"sw\", \"te\", \"th\", \"zh\"]\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "# Iterate over each language and evaluate\n",
        "for lang in languages:\n",
        "    print(f\"\\nProcessing language: {lang}\")\n",
        "    dataset = load_dataset(\"miracl/miracl\", lang, split=\"dev\")  # Load dataset\n",
        "    recall, ndcg = evaluate_miracl(dataset)  # Evaluate\n",
        "    results[lang] = {\"Recall@100\": recall, \"nDCG@10\": ndcg}"
      ],
      "metadata": {
        "id": "1uk5bcNK4e-m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}