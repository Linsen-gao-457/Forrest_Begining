{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_miracl(dataset):\n",
    "    print(f\"\\nEvaluating language: {lang}...\")\n",
    "\n",
    "    # Extract queries\n",
    "    queries = dataset[\"query\"][:200]  # Take first 200 queries\n",
    "    query_ids = dataset[\"query_id\"][:200]\n",
    "\n",
    "    # Extract positive and negative passages\n",
    "    all_positive_passages = [p[0] if p else None for p in dataset[\"positive_passages\"]]\n",
    "    all_negative_passages = [n[0] if n else None for n in dataset[\"negative_passages\"]] \n",
    "\n",
    "    # Combine positive + negative passages to form the full corpus\n",
    "    corpus = list(set(all_positive_passages + all_negative_passages))  # Remove duplicates\n",
    "    corpus = [p for p in corpus if p is not None]  # Remove None values\n",
    "\n",
    "    if not queries or not corpus:\n",
    "        print(f\"No valid data found for {lang}\")\n",
    "        return 0, 0\n",
    "\n",
    "    # Create mapping of query â†’ relevant positive passages\n",
    "    query_to_positive = {\n",
    "        q: set(p) for q, p in zip(dataset[\"query\"], dataset[\"positive_passages\"]) if p\n",
    "    }\n",
    "    \n",
    "    # Embed queries\n",
    "    query_embeddings = get_embedding(queries)\n",
    "\n",
    "    # Embed the full corpus\n",
    "    corpus_embeddings = get_embedding(corpus)\n",
    "\n",
    "    # Build FAISS index for full corpus\n",
    "    dim = corpus_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(corpus_embeddings)\n",
    "\n",
    "    # Retrieve top-k results from the full corpus\n",
    "    k_recall, k_ndcg = 100, 10\n",
    "    D, I = index.search(query_embeddings, max(k_recall, k_ndcg))\n",
    "\n",
    "    # Extract positive passage sets for each query\n",
    "    query_to_positive = {\n",
    "        q: set(p) for q, p in zip(dataset[\"query\"], dataset[\"positive_passages\"]) if p\n",
    "    }\n",
    "\n",
    "    # Compute Recall@100\n",
    "    recall_count = sum(\n",
    "        1 for i, query in enumerate(queries)\n",
    "        if query_to_positive.get(query, set()) & {corpus[idx] for idx in I[i][:k_recall]}\n",
    "    )\n",
    "    recall_100 = recall_count / len(queries)\n",
    "    \n",
    "    # Compute NDCG@10\n",
    "    true_relevance = np.zeros((len(queries), k_ndcg))\n",
    "    for i, query in enumerate(queries):\n",
    "        relevant_docs = query_to_positive.get(query, set())\n",
    "        for j, retrieved_idx in enumerate(I[i][:k_ndcg]):\n",
    "            if corpus[retrieved_idx] in relevant_docs:\n",
    "                true_relevance[i, j] = 1  # Assign relevance score\n",
    "    \n",
    "    ndcg_10 = np.mean([\n",
    "        ndcg_score([true_relevance[i]], [I[i][:k_ndcg]]) for i in range(len(queries))\n",
    "    ])\n",
    "    \n",
    "    print(f\"{lang} - Recall@100: {recall_100:.4f}, NDCG@10: {ndcg_10:.4f}\")\n",
    "    return recall_100, ndcg_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load KaLM multilingual embedding model\n",
    "model_name = \"HIT-TMG/KaLM-embedding-multilingual-mini-v1\"\n",
    "model = SentenceTransformer(model_name).to(device)\n",
    "\n",
    "# Define function to get sentence embeddings\n",
    "def get_embedding(texts):\n",
    "    return model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "\n",
    "# Load the MIRACL Yoruba (yo) corpus\n",
    "miracl_corpus = load_dataset(\"miracl/miracl-corpus\", \"yo\")  # Load Yoruba split\n",
    "\n",
    "# Extract passages from the corpus (FAISS index will be built on these)\n",
    "corpus = miracl_corpus[\"train\"][\"passage\"]  # The full set of passages in Yoruba\n",
    "\n",
    "# Load queries and positive passages for Yoruba from MIRACL Query Dataset\n",
    "miracl_queries = load_dataset(\"miracl/miracl\", \"yo\")  # Load Yoruba query dataset\n",
    "\n",
    "def evaluate_miracl(dataset):\n",
    "    print(\"\\nEvaluating Yoruba (yo) Language...\")\n",
    "\n",
    "    # Extract queries and corresponding positive passages\n",
    "    num_queries = min(1000, len(dataset[\"query\"]))  # Use up to 1000 queries\n",
    "    queries = dataset[\"query\"][:num_queries]\n",
    "\n",
    "    # Extract positive passages mapping\n",
    "    query_to_positive = {\n",
    "        q: set(p) for q, p in zip(dataset[\"query\"], dataset[\"positive_passages\"]) if p\n",
    "    }\n",
    "\n",
    "    if not queries or not corpus:\n",
    "        print(\"No valid data found for Yoruba (yo)\")\n",
    "        return 0, 0\n",
    "\n",
    "    # Generate embeddings for queries and full corpus\n",
    "    query_embeddings = get_embedding(queries)\n",
    "    corpus_embeddings = get_embedding(corpus)\n",
    "\n",
    "    # Build FAISS index using full Yoruba corpus\n",
    "    dim = corpus_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(corpus_embeddings)\n",
    "\n",
    "    # Retrieve top-k results\n",
    "    k_recall, k_ndcg = 100, 10\n",
    "    D, I = index.search(query_embeddings, max(k_recall, k_ndcg))\n",
    "\n",
    "    # Compute Recall@100\n",
    "    recall_count = 0\n",
    "    for i, query in enumerate(queries):\n",
    "        retrieved_docs = {corpus[idx] for idx in I[i][:k_recall]}  # Get top-k retrieved passages\n",
    "        relevant_docs = query_to_positive.get(query, set())  # Get actual relevant passages\n",
    "\n",
    "        if len(relevant_docs & retrieved_docs) > 0:  # Check if any relevant passage is retrieved\n",
    "            recall_count += 1\n",
    "\n",
    "    recall_100 = recall_count / len(queries)\n",
    "\n",
    "    # Compute NDCG@10\n",
    "    true_relevance = np.zeros((len(queries), k_ndcg))\n",
    "    for i, query in enumerate(queries):\n",
    "        relevant_docs = query_to_positive.get(query, set())\n",
    "\n",
    "        for j, retrieved_idx in enumerate(I[i][:k_ndcg]):\n",
    "            if corpus[retrieved_idx] in relevant_docs:\n",
    "                true_relevance[i, j] = 1  # Assign relevance score\n",
    "\n",
    "    ndcg_10 = np.mean([ndcg_score([rel], [rank]) for rel, rank in zip(true_relevance, I[:, :k_ndcg])])\n",
    "\n",
    "    print(f\"Yoruba (yo) - Recall@100: {recall_100:.4f}, NDCG@10: {ndcg_10:.4f}\")\n",
    "    return recall_100, ndcg_10\n",
    "\n",
    "\n",
    "# Run evaluation on Yoruba queries\n",
    "evaluate_miracl(miracl_queries[\"dev\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load mContriever model from Hugging Face\n",
    "model_name = \"facebook/mcontriever-msmarco\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Define function to get sentence embeddings using mContriever\n",
    "def get_embedding(texts, batch_size=16):\n",
    "    \"\"\"\n",
    "    Compute embeddings using Facebook's mContriever model.\n",
    "    - Splits texts into batches for efficient processing.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs).last_hidden_state  # Extract last hidden state\n",
    "            batch_embeddings = outputs[:, 0, :].cpu().numpy()  # Use CLS token embeddings\n",
    "            embeddings.append(batch_embeddings)\n",
    "    \n",
    "    return np.vstack(embeddings)  # Stack all batch embeddings together\n",
    "\n",
    "\n",
    "# Load the MIRACL Yoruba (yo) corpus\n",
    "miracl_corpus = load_dataset(\"miracl/miracl-corpus\", \"yo\")  # Load Yoruba split\n",
    "\n",
    "# Extract passages from the corpus (FAISS index will be built on these)\n",
    "corpus = miracl_corpus[\"dev\"][\"passage\"]  # The full set of passages in Yoruba\n",
    "\n",
    "# Load queries and positive passages for Yoruba from MIRACL Query Dataset\n",
    "miracl_queries = load_dataset(\"miracl/miracl\", \"yo\")  # Load Yoruba query dataset\n",
    "\n",
    "def evaluate_miracl(dataset):\n",
    "    print(\"\\nEvaluating Yoruba (yo) Language using mContriever...\")\n",
    "\n",
    "    # Extract queries and corresponding positive passages\n",
    "    num_queries = min(1000, len(dataset[\"query\"]))  # Use up to 1000 queries\n",
    "    queries = dataset[\"query\"][:num_queries]\n",
    "\n",
    "    # Extract positive passages mapping\n",
    "    query_to_positive = {\n",
    "        q: set(p) for q, p in zip(dataset[\"query\"], dataset[\"positive_passages\"]) if p\n",
    "    }\n",
    "\n",
    "    if not queries or not corpus:\n",
    "        print(\"No valid data found for Yoruba (yo)\")\n",
    "        return 0, 0\n",
    "\n",
    "    # Generate embeddings for queries and full corpus using mContriever\n",
    "    query_embeddings = get_embedding(queries)\n",
    "    corpus_embeddings = get_embedding(corpus)\n",
    "\n",
    "    # Build FAISS index using full Yoruba corpus\n",
    "    dim = corpus_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(corpus_embeddings)\n",
    "\n",
    "    # Retrieve top-k results\n",
    "    k_recall, k_ndcg = 100, 10\n",
    "    D, I = index.search(query_embeddings, max(k_recall, k_ndcg))\n",
    "\n",
    "    # Compute Recall@100\n",
    "    recall_count = 0\n",
    "    for i, query in enumerate(queries):\n",
    "        retrieved_docs = {corpus[idx] for idx in I[i][:k_recall]}  # Get top-k retrieved passages\n",
    "        relevant_docs = query_to_positive.get(query, set())  # Get actual relevant passages\n",
    "\n",
    "        if len(relevant_docs & retrieved_docs) > 0:  # Check if any relevant passage is retrieved\n",
    "            recall_count += 1\n",
    "\n",
    "    recall_100 = recall_count / len(queries)\n",
    "\n",
    "    # Compute NDCG@10\n",
    "    true_relevance = np.zeros((len(queries), k_ndcg))\n",
    "    for i, query in enumerate(queries):\n",
    "        relevant_docs = query_to_positive.get(query, set())\n",
    "\n",
    "        for j, retrieved_idx in enumerate(I[i][:k_ndcg]):\n",
    "            if corpus[retrieved_idx] in relevant_docs:\n",
    "                true_relevance[i, j] = 1  # Assign relevance score\n",
    "\n",
    "    ndcg_10 = np.mean([ndcg_score([rel], [rank]) for rel, rank in zip(true_relevance, I[:, :k_ndcg])])\n",
    "\n",
    "    print(f\"Yoruba (yo) - Recall@100: {recall_100:.4f}, NDCG@10: {ndcg_10:.4f}\")\n",
    "    return recall_100, ndcg_10\n",
    "\n",
    "\n",
    "# Run evaluation on Yoruba queries using mContriever\n",
    "evaluate_miracl(miracl_queries[\"dev\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
